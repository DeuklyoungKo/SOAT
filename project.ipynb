{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa225bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lpips\n",
    "from model import Generator\n",
    "import numpy as np\n",
    "from util import *\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342fb8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_loss(v):\n",
    "    # [B, 9088]\n",
    "    loss = (v-gt_mean) @ gt_cov_inv @ (v-gt_mean).transpose(1,0)\n",
    "    return loss.mean()\n",
    "\n",
    "def get_lr(t, initial_lr, rampdown=0.25, rampup=0.05):\n",
    "    lr_ramp = min(1, (1 - t) / rampdown)\n",
    "    lr_ramp = 0.5 - 0.5 * math.cos(lr_ramp * math.pi)\n",
    "    lr_ramp = lr_ramp * min(1, t / rampup)\n",
    "\n",
    "    return initial_lr * lr_ramp\n",
    "\n",
    "def make_image(tensor):\n",
    "    return (\n",
    "        tensor.detach()\n",
    "        .clamp_(min=-1, max=1)\n",
    "        .add(1)\n",
    "        .div_(2)\n",
    "        .mul(255)\n",
    "        .type(torch.uint8)\n",
    "        .permute(0, 2, 3, 1)\n",
    "        .to('cpu')\n",
    "        .numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "602b2bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = 256\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(resize),\n",
    "        transforms.CenterCrop(resize),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "093a2c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = 'cuda'\n",
    "device = 'cpu'\n",
    "\n",
    "#imgfile = 'imgs/01.jpg'\n",
    "imgfile = 'imgs/02.jpg'\n",
    "imgs = []\n",
    "\n",
    "img = transform(Image.open(imgfile).convert('RGB'))\n",
    "\n",
    "print(\"img.shape:\",img.shape)\n",
    "\n",
    "imgs.append(img)\n",
    "\n",
    "imgs = torch.stack(imgs, 0).to(device)\n",
    "\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f08a2bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ema = Generator(resize, 512, 8)\n",
    "ensure_checkpoint_exists('face.pt')\n",
    "g_ema.load_state_dict(torch.load('face.pt')['g_ema'], strict=False)\n",
    "g_ema = g_ema.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a3a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    latent_mean = g_ema.mean_latent(50000)\n",
    "    latent_in = list2style(latent_mean)\n",
    "\n",
    "# get gaussian stats\n",
    "if not os.path.isfile('inversion_stats.npz'):\n",
    "    with torch.no_grad():\n",
    "#         source = list2style(g_ema.get_latent(torch.randn([10000, 512]).cuda())).cpu().numpy()\n",
    "        source = list2style(g_ema.get_latent(torch.randn([10000, 512]))).cpu().numpy()\n",
    "        gt_mean = source.mean(0)\n",
    "        gt_cov = np.cov(source, rowvar=False)\n",
    "\n",
    "    # We show that style space follows gaussian distribution\n",
    "    # An extension from this work https://arxiv.org/abs/2009.06529\n",
    "    np.savez('inversion_stats.npz', mean=gt_mean, cov=gt_cov)\n",
    "\n",
    "data = np.load('inversion_stats.npz')\n",
    "# gt_mean = torch.tensor(data['mean']).cuda().view(1,-1).float()\n",
    "gt_mean = torch.tensor(data['mean']).view(1,-1).float()\n",
    "# gt_cov_inv = torch.tensor(data['cov']).cuda()\n",
    "gt_cov_inv = torch.tensor(data['cov'])\n",
    "\n",
    "# Only take diagonals\n",
    "# mask = torch.eye(*gt_cov_inv.size()).cuda()\n",
    "mask = torch.eye(*gt_cov_inv.size())\n",
    "gt_cov_inv = torch.inverse(gt_cov_inv*mask).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f900cc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [on]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Work\\220119_GAN\\Controllable Toonification using Pretrained StyleGAN2\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "D:\\Work\\220119_GAN\\Controllable Toonification using Pretrained StyleGAN2\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: D:\\Work\\220119_GAN\\Controllable Toonification using Pretrained StyleGAN2\\venv\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n"
     ]
    }
   ],
   "source": [
    "step = 3000\n",
    "lr = 1.0\n",
    "\n",
    "percept = lpips.LPIPS(net='vgg', spatial=True).to(device)\n",
    "latent_in.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam([latent_in], lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b08542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 16.7499; perceptual: 10.2681; mse: 0.1686; gaussian: 6.3132 lr: 1.0000:   0%|                                                                                  | 3/3000 [00:09<2:39:40,  3.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12252\\322173734.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\220119_GAN\\Controllable Toonification using Pretrained StyleGAN2\\venv\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\220119_GAN\\Controllable Toonification using Pretrained StyleGAN2\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_latent = None\n",
    "min_loss = 100\n",
    "pbar = tqdm(range(step))\n",
    "latent_path = []\n",
    "\n",
    "for i in pbar:\n",
    "    t = i / step\n",
    "#     lr = get_lr(t, lr)\n",
    "    if i > 0 and i % 500 == 0:\n",
    "        lr *= 0.2\n",
    "    latent_n = latent_in\n",
    "\n",
    "    img_gen, _ = g_ema(style2list(latent_n))\n",
    "\n",
    "    batch, channel, height, width = img_gen.shape\n",
    "\n",
    "    if height > 256:\n",
    "        img_gen = F.interpolate(img_gen, size=(256,256), mode='area')\n",
    "\n",
    "    p_loss = 20*percept(img_gen, imgs).mean()\n",
    "    mse_loss = 1*F.mse_loss(img_gen, imgs)\n",
    "    g_loss = 1e-3*gaussian_loss(latent_n)\n",
    "\n",
    "    loss = p_loss + mse_loss + g_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        latent_path.append(latent_in.detach().clone())\n",
    "        \n",
    "    if loss.item() < min_loss:\n",
    "        min_loss = loss.item()\n",
    "        min_latent = latent_in.detach().clone()\n",
    "\n",
    "    pbar.set_description(\n",
    "        (\n",
    "            f'loss: {loss.item():.4f}; '\n",
    "            f'perceptual: {p_loss.item():.4f}; '\n",
    "            f'mse: {mse_loss.item():.4f}; gaussian: {g_loss.item():.4f} lr: {lr:.4f}'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "latent_path.append(latent_in.detach().clone()) # last latent vector\n",
    "\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gen, _ = g_ema(style2list(latent_path[-1]))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(make_image(img_gen)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490682f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gen, _ = g_ema(style2list(min_latent))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(make_image(img_gen)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d622c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'latent': min_latent}, 'inversion_codes/01.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e133c7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
